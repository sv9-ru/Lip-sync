# Проект: Синхронизация губ (Lip-sync) с Wav2Lip и GFPGAN
  
Данный проект посвящен работе с нейросетями для синхронизации мимики лица с аудиодорожкой.  

**Используемые модели**  
* Wav2Lip — базовая модель синхронизации.  
* Wav2Lip-GFPGAN — улучшенная версия с реставрацией лиц.  
  
**Подготовка данных**
* используется запись говорящего человека (лицо четко в кадре, хорошее освещение).  
* 3 аудиодорожки (10-20 сек):

> Нативная английская речь.

> Нативная русская речь.

> Синтезированная речь (из задания по TTS).

## Вывод

Сравнение качества изображения (Wav2Lip vs Wav2Lip-GFPGAN)

Wav2Lip

Нижняя половина лица (вокруг рта) более размытая, чем остальная часть видео. Это связано с тем, что модель генерирует кроп 96x96 пикселей, который затем растягивается обратно на исходное видео. Границы вклейки могут быть заметны.
  
![Видео](https://github.com/sv9-ru/Lip-sync/blob/main/results_Wav2Lip/ru_native_Wav2Lip_fixed.mp4) 

Wav2Lip-GFPGAN

GFPGAN - это реставратор лиц. Он берет размытый результат Wav2Lip и "дорисовывает" детали: текстуру кожи, зубы, четкость губ. Картинка выглядит намного приятнее и четче, разрешение соответствует исходному видео.

![Видео](https://github.com/sv9-ru/Lip-sync/blob/main/results_GFPGAN/ru_native_HD_result.avi)

Использование Wav2Lip-GFPGAN значительно повышает визуальное восприятие видео за счет устранения размытости в области рта. Лучшая синхронизация губ (по таймингу) наблюдается на нативной английской речи, так как модель, вероятно, обучалась на похожих данных.
